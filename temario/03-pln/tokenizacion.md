# Tokenización y normalización

## ¿Por qué?

Computadoras no "leen" como humanos. Necesitan texto estructurado: palabra por palabra, oración por oración.

## ¿Qué?

Tokenización = segmentar texto en unidades (palabras, oraciones). Normalización = convertir a forma estándar (minúsculas, sin acentos, lematización).

## ¿Para qué?

Paso previo obligatorio para cualquier análisis automatizado. Análisis de frecuencias. Comparación de corpus.

## ¿Cómo?

- Demo spaCy: texto → tokens → lemas
- Ejercicio: Comparar frecuencia léxica en dos corpus (formal vs coloquial)
- Notebook pre-configurado: cambiar texto, ver output
